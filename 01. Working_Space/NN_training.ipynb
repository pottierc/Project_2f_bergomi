{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "309b3cf6",
   "metadata": {},
   "source": [
    "<header style=\"background-color: rgb(0, 62, 92); color: white; margin-top: 20px; padding:28px; \">\n",
    "  <img src=\"images/Xlogo.png\" alt=\"Transposition of a vector\" title=\"Vector transposition\" width=\"115\" style=\"float: left;\">\n",
    "  <p style=\" text-align: center; font-size: 32px;\">   \n",
    "   <strong> Deep Learning in Finance MAP548 </strong></p>\n",
    "  <p style=\" text-align: center; font-size: 25px;\"><strong> Project 1 -  Deep pricing and calibration </strong></p>\n",
    "  <p style=\" text-align: center; font-size: 20px;\"> Eduardo Abi Jaber </p>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e988aa5",
   "metadata": {},
   "source": [
    "# <font color='red'>PLEASE ENTER YOUR FULL NAMES HERE:</font>\n",
    "\n",
    "\n",
    "\n",
    "- MANSARD Diane\n",
    "- POTTIER Clément\n",
    "\n",
    "<font color='red'>**DEADLINE: March 2 (5:00 pm)**</font>\n",
    "\n",
    "<font color='red'>**PLease send both pdf ipynb files with name : Name1_Name2_Project1**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ad6071",
   "metadata": {},
   "source": [
    "# The two factor Bergomi model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09088bf8-62fe-49d7-91f7-359286786616",
   "metadata": {},
   "source": [
    "The two factor Bergomi Model (L Bergomi, 2005) under the risk-neutral filtered probability space $(\\Omega, \\mathcal F,(\\mathcal F_t)_{t\\geq 0}, \\mathbb Q )$  has the following dynamics:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "  \\begin{aligned}\n",
    "    dS_t &= S_t\\sqrt{V_t}dB_t, \\quad S_0>0;\\\\\n",
    "    X_t &= X_t^1 + X_t^2,\\\\\n",
    "    V_t &= \\xi \\exp{\\left(X_t-\\frac{1}{2} \\mathbb{V}[X_t] \\right)},\\\\\n",
    "      X_t^i &=  \\eta_i\\int_0^t e^{-\\kappa_i(t-s)} dW_s,\n",
    "  \\end{aligned}\n",
    "  \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf94875-3111-407c-9e8f-3530c5d5338b",
   "metadata": {},
   "source": [
    "where $ B=\\rho W + \\sqrt{1-\\rho^2} W^{\\perp}$ with $(W,W^{\\perp})$ a two-dimensional Brownian motion, $\\rho \\in [-1,1]$.\n",
    "\n",
    "$X_t^i$ is a Ornstein–Uhlenbeck (Gaussian) process such that $X_t^i \\sim \\mathcal{N}(0, \\eta_i^2\\frac{(1-e^{-2\\kappa_i t})}{2\\kappa_i})$. Note that both $X^1$ and $X^2$ are driven by the same Brownian motion.\n",
    "\n",
    "Recall $\\mathbb V[(X+Y)] = \\mathbb V[X] + \\mathbb V[Y] + 2\\mathbb {COV}[X,Y]$.\n",
    "\n",
    "In addition, we fix $\\kappa_2=2.6$, thus there are in total five calibratable model parameters: $(\\xi, \\kappa_1, \\eta_1, \\eta_2, \\rho)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c743c4e-a7e7-4910-ad27-341afb1074d6",
   "metadata": {},
   "source": [
    "We are interested in computing the price of European style contingent claims, with payoff $g(S_T)$ for some function $g$:\n",
    "$$\n",
    "C_t = \\mathbb{E} \\left[g(S_T) \\vert \\mathcal F_t) \\right].\n",
    "$$\n",
    "\n",
    "For European vanilla call options, with $g(x) = (x-K)^{+}$ with strike $K$; no closed form formula to compute $g(S_T)$\n",
    "\n",
    "Suggested range of model parameters for training:\n",
    "\n",
    "$\\xi \\in [0.03,0.25], \\kappa_1 \\in [10,60], \\eta_1 \\in [5,35],\\eta_2 \\in [1,5],\\rho \\in [-0.9,-0.1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24e44a4",
   "metadata": {},
   "source": [
    "# Your task\n",
    "\n",
    "Inspired by the deep pricer for the one factor Bergomi model. Implement a deep pricer for the two factor Bergomi model. You have to generate your own training set using a scheme of your choice that you have to detail (use the fixed grid for strikes and maturities below).\n",
    "\n",
    "Once your NN is trained, showcase the train and test error, and perform a calibration on the market implied volatility surface that was used in the one factor Bergomi and comment. \n",
    "\n",
    "Also provide/display the output prices of the NN of the set of parameters set 1, 2 and 3 below. \n",
    "\n",
    "You have to provide a notebook that compiles, together with the trained weights of your Neural Networks that we can load with the command: *model_iv.load_weights('2FBergomiNNWeights.h5')*. \n",
    "\n",
    "(!) The actual training of NN is not difficult, the difficult part is to get good data (and lots of it), so be careful about your simulation schemes (training might require a larger dataset than for 1 factor bergomi)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6e8bb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7181698a",
   "metadata": {},
   "source": [
    "## Let's generate the dataset with moving parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fc9ab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=1000\n",
    "N_sims = 50\n",
    "kappa_2 = 2.\n",
    "T = 2\n",
    "n_steps = 1000\n",
    "dt = T / n_steps\n",
    "tt = np.linspace(0, T, n_steps + 1)\n",
    "strikes = np.array([0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5])\n",
    "indices = {time: np.argmin(np.abs(tt - time)) for time in [0.1, 0.3, 0.6, 0.9, 1.2, 1.5, 1.8, 2.0]}\n",
    "S0 = 100\n",
    "r = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df4fd618",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "def gen_bm_path(n_steps,N_sims):\n",
    "    w1 = np.random.normal(0, 1, (n_steps, N_sims))\n",
    "    return w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f7d514b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac15fa97bfd4f9b80bd315ddc1c7930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Avancement:   0%|          | 0/1000 [00:00<?, ?iter/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progression : 100.00%"
     ]
    }
   ],
   "source": [
    "X_data = np.array([]).reshape(0, 5)\n",
    "y_data = np.array([]).reshape(0, 88)\n",
    "\n",
    "for i in tqdm(range(N), desc=\"Avancement\", unit=\"iter\"):\n",
    "    xi = np.random.uniform(0.03, 0.25)\n",
    "    kappa_1 = np.random.uniform(10, 60)\n",
    "    eta_1 = np.random.uniform(5, 35)\n",
    "    eta_2 = np.random.uniform(1, 5)\n",
    "    rho = np.random.uniform(-0.9, -0.1)\n",
    "    \n",
    "    \n",
    "    w1 = gen_bm_path(n_steps, N_sims)\n",
    "    w2 = gen_bm_path(n_steps, N_sims)\n",
    "    log_S = np.ones(N_sims).reshape(1,-1) * np.log(S0)\n",
    "    \n",
    "    log_S = np.ones(N_sims).reshape(1,-1) * np.log(S0)\n",
    "    \n",
    "    # sim Xt1\n",
    "    exp1_Xt1 = np.exp(kappa_1*tt[1:])\n",
    "    exp2_Xt1 = np.exp(2*kappa_1*tt)\n",
    "\n",
    "    diff_exp2_Xt1 = np.diff(exp2_Xt1)\n",
    "    std_vec_Xt1 = np.sqrt(diff_exp2_Xt1/(2*kappa_1))[:,np.newaxis] #to be broadcasted columnwise \n",
    "    exp1_Xt1 = exp1_Xt1[:,np.newaxis]\n",
    "    Xt1 = np.concatenate((np.zeros(w1.shape[1])[np.newaxis, :],(1/exp1_Xt1)*np.cumsum(std_vec_Xt1*w1, axis = 0)))\n",
    "    Xt1 = eta_1*Xt1\n",
    "    \n",
    "    # sim Xt2\n",
    "    exp1_Xt2 = np.exp(kappa_2*tt[1:])\n",
    "    exp2_Xt2 = np.exp(2*kappa_2*tt)\n",
    "\n",
    "    diff_exp2_Xt2 = np.diff(exp2_Xt2)\n",
    "    std_vec_Xt2 = np.sqrt(diff_exp2_Xt2/(2*kappa_2))[:,np.newaxis] #to be broadcasted columnwise \n",
    "    exp1_Xt2 = exp1_Xt2[:,np.newaxis]\n",
    "    Xt2 = np.concatenate((np.zeros(w1.shape[1])[np.newaxis, :],(1/exp1_Xt2)*np.cumsum(std_vec_Xt2*w1, axis = 0)))\n",
    "    Xt2 = eta_2*Xt2\n",
    "    \n",
    "    Xt = Xt1 + Xt2\n",
    "    \n",
    "    #calcul de Vt\n",
    "    var1 = eta_1**2*(1-np.exp(-2*kappa_1*tt))/(2*kappa_1)\n",
    "    var2 = eta_2**2*(1-np.exp(-2*kappa_2*tt))/(2*kappa_2)\n",
    "    covar = eta_1*eta_2*(1-np.exp(-(kappa_1+kappa_2)*tt))/(kappa_1+kappa_2) \n",
    "    var = var1 + var2 + 2*covar\n",
    "\n",
    "    Vt = xi * np.exp(Xt - 0.5 * var[:, np.newaxis])\n",
    "    \n",
    "    for j in range(n_steps):\n",
    "        log_S_next = log_S[j] - 0.5 * Vt[j] * dt + np.sqrt(Vt[j] * dt) * (rho * w1[j] + np.sqrt(1 - rho**2) * w2[j])\n",
    "        log_S = np.append(log_S, log_S_next.reshape(1,-1), axis=0)\n",
    "    \n",
    "    S = np.exp(log_S)\n",
    "    prices = {}\n",
    "    for time, index in indices.items():\n",
    "        prices[time] = [np.exp(-r * (T - time)) * np.mean(np.maximum(S[index] - strike, 0)) for strike in strikes]\n",
    "    \n",
    "    values_list = [value for sublist in prices.values() for value in sublist]\n",
    "    y = np.array(values_list)\n",
    "    X = np.array([xi, kappa_1, eta_1, eta_2, rho]).reshape(1, -1)\n",
    "    #if i == 0:\n",
    "    #    X_data = X\n",
    "    #    y_data = y.reshape(1, -1)\n",
    "    #else:\n",
    "    X_data = np.append(X_data, X, axis=0)\n",
    "    y_data = np.append(y_data, y.reshape(1, -1), axis=0)\n",
    "    pourcentage = (i + 1) / N * 100\n",
    "    print(f\"\\rProgression : {pourcentage:.2f}%\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32e0b60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 5)\n",
      "(1000, 88)\n"
     ]
    }
   ],
   "source": [
    "print(X_data.shape)\n",
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3154cd0-fb0c-4c3c-bfba-d89fb82080cb",
   "metadata": {},
   "source": [
    "# Sample IV surface values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df287b83-4008-43fc-b01c-12623be0c1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on the grid:\n",
    "S0 = 100\n",
    "strikes=np.array([0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3,1.4,1.5 ])*S0\n",
    "maturities=np.array([0.1,0.3,0.6,0.9,1.2,1.5,1.8,2.0 ])\n",
    "\n",
    "indices = {time: int(time / dt) for time in maturities}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa4b796-4543-4277-b63f-aeebd871fb0e",
   "metadata": {},
   "source": [
    "### parameter set 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeea4dc-056c-464e-804e-f8143d046b84",
   "metadata": {},
   "source": [
    "$\\xi = 0.06, \\kappa_1 =41.6, \\eta_1 = 18.2428, \\eta_2 = 3.43, \\rho = -0.7$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35a86fc-81b5-4466-b8fd-529c221935cf",
   "metadata": {},
   "source": [
    "### parameter set 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aa2d7b-9991-4775-aa1c-b1a8ba88019b",
   "metadata": {},
   "source": [
    "$\\xi = 0.1, \\kappa_1 = 15.6, \\eta_1 = 5.5857, \\eta_2 = 2.2867, \\rho = -0.4$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b0ff93-f0fd-439e-96ec-ded174ca67ac",
   "metadata": {},
   "source": [
    "### parameter set 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15081896-4753-4e26-b4e4-1c0de7bded9c",
   "metadata": {},
   "source": [
    "$\\xi = 0.2, \\kappa_1 = 54.6, \\eta_1 = 31.3496, \\eta_2 = 4.5733, \\rho = -0.8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4032828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e56e717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_params(x):\n",
    "    print(f'xi = {x[0]}', '\\n', \n",
    "          f'kappa_1 = {x[1]}', '\\n', \n",
    "          f'eta_1 = {x[2]}', '\\n',\n",
    "          f'eta_2 = {x[3]}', '\\n',\n",
    "          f'rho = {x[4]}', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0222ef0f",
   "metadata": {},
   "source": [
    "## Preparation of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc50cbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1000, 5)\n",
      "Y shape: (1000, 88)\n"
     ]
    }
   ],
   "source": [
    "#training_data = # importer les données d'entraînement\n",
    "X = X_data\n",
    "#X=training_data[:,:4]\n",
    "target_y = y_data\n",
    "#target_y=training_data[:,4:]\n",
    "#print(training_data.shape)\n",
    "print('X shape:', X.shape)\n",
    "print('Y shape:', target_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f80757a",
   "metadata": {},
   "source": [
    "Split the Data between training (85%) and testing (15%) and normalise inputs between $[-1,1]$ and center/standardise outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8c69530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data to training and testing set\n",
    "X_indexed = np.concatenate((np.arange(X.shape[0])[np.newaxis].T,X),axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_indexed, target_y, test_size=0.15, random_state=42)\n",
    "\n",
    "train_index = X_train[:,0].astype(int)\n",
    "test_index = X_test[:,0].astype(int)\n",
    "X_train = X_train[:,1:]\n",
    "X_test = X_test[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10d426a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalising input data x to the interval of [-1, 1]\n",
    "def input_norm_transform(x, x_min, x_max, norm_min, norm_max):\n",
    "    return (x-x_min)/(x_max-x_min)*(norm_max-norm_min)+norm_min\n",
    "def input_norm_transform_inv(x_norm, x_min, x_max, norm_min, norm_max):\n",
    "    return (x_norm-norm_min)/(norm_max-norm_min)*(x_max-x_min)+x_min\n",
    "\n",
    "x_max = np.array([np.max(X_train[:,0]),  np.max(X_train[:,1]),  np.max(X_train[:,2]), np.max(X_train[:,3]), np.max(X_train[:,4])])\n",
    "x_min = np.array([np.min(X_train[:,0]),  np.min(X_train[:,1]),  np.min(X_train[:,2]), np.min(X_train[:,3]), np.min(X_train[:,4])])\n",
    "x_norm_min = -1\n",
    "x_norm_max = 1\n",
    "\n",
    "x_train_transform = input_norm_transform(X_train, x_min, x_max, x_norm_min, x_norm_max)\n",
    "x_test_transform = input_norm_transform(X_test, x_min, x_max, x_norm_min, x_norm_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "969dc189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_max: [ 0.24997966 59.98343366 34.94671874  4.99627174 -0.10019679]\n",
      "x_min: [ 0.03073441 10.07276595  5.09257463  1.00006501 -0.89909073]\n"
     ]
    }
   ],
   "source": [
    "#Let's check the range for each parameter:\n",
    "print('x_max:', x_max)\n",
    "print('x_min:', x_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "063f0feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardising output data y to the interval, so it is centred with variance 1\n",
    "def output_norm_transform(y, mean, std):\n",
    "    return (y-mean)/std\n",
    "def output_norm_transform_inv(y_trans, mean, std):\n",
    "    return y_trans*std+mean\n",
    "\n",
    "y_mean = np.average(y_train,axis=0)\n",
    "y_std = np.std(y_train,axis=0)\n",
    "\n",
    "y_train_transform = output_norm_transform(y_train, y_mean, y_std)\n",
    "y_test_transform = output_norm_transform(y_test, y_mean, y_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44a6ea5",
   "metadata": {},
   "source": [
    "## Building of the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d430720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">360</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,660</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,660</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,660</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,660</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">88</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,368</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m)             │           \u001b[38;5;34m360\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m)             │         \u001b[38;5;34m3,660\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m)             │         \u001b[38;5;34m3,660\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m)             │         \u001b[38;5;34m3,660\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m)             │         \u001b[38;5;34m3,660\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m88\u001b[0m)             │         \u001b[38;5;34m5,368\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,368</span> (79.56 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,368\u001b[0m (79.56 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,368</span> (79.56 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m20,368\u001b[0m (79.56 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_iv = keras.models.Sequential([\n",
    "    keras.layers.Input(shape=(5,)),\n",
    "    keras.layers.Dense(60, activation=\"elu\"),\n",
    "    keras.layers.Dense(60, activation=\"elu\"),\n",
    "    keras.layers.Dense(60, activation=\"elu\"),\n",
    "    keras.layers.Dense(60, activation=\"elu\"),\n",
    "    keras.layers.Dense(60, activation=\"elu\"),\n",
    "    keras.layers.Dense(y_train_transform.shape[1], activation=\"linear\")\n",
    "])\n",
    "\n",
    "model_iv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c2a3f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.9486\n",
      "Epoch 2/20\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0113\n",
      "Epoch 3/20\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9323\n",
      "Epoch 4/20\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9726\n",
      "Epoch 5/20\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9675\n",
      "Epoch 6/20\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9655\n",
      "Epoch 7/20\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9855\n",
      "Epoch 8/20\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9618\n",
      "Epoch 9/20\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9586\n",
      "Epoch 10/20\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9971\n",
      "Epoch 11/20\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0122\n",
      "Epoch 12/20\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0198\n",
      "Epoch 13/20\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9982\n",
      "Epoch 14/20\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9790\n",
      "Epoch 15/20\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0248\n",
      "Epoch 16/20\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9812\n",
      "Epoch 17/20\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9528\n",
      "Epoch 18/20\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0200\n",
      "Epoch 19/20\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9885\n",
      "Epoch 20/20\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0132\n"
     ]
    }
   ],
   "source": [
    "def error_function(y_true, y_pred):\n",
    "    return tf.sqrt(tf.math.reduce_mean((y_pred - y_true) ** 2))\n",
    "\n",
    "# Define the optimizer with a specific learning rate\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "model_iv.compile(optimizer=\"adam\", loss=error_function)\n",
    "train_hist = model_iv.fit(x_train_transform, y_train_transform, batch_size=16, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3008dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 97.8410  \n",
      "Test Loss: 97.72058868408203\n"
     ]
    }
   ],
   "source": [
    "test_loss = model_iv.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2f9a2a3-19fc-4b78-a49a-5380fcccb391",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to open file (unable to open file: name = '2FBergomiNNWeights_test.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_iv\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2FBergomiNNWeights_test.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\cleme\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\cleme\\anaconda3\\Lib\\site-packages\\h5py\\_hl\\files.py:562\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[0;32m    553\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[0;32m    554\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[0;32m    555\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[0;32m    556\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[0;32m    557\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[0;32m    558\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    559\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[0;32m    560\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[0;32m    561\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[1;32m--> 562\u001b[0m     fid \u001b[38;5;241m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[38;5;241m=\u001b[39mswmr)\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[1;32mc:\\Users\\cleme\\anaconda3\\Lib\\site-packages\\h5py\\_hl\\files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[0;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[1;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, flags, fapl\u001b[38;5;241m=\u001b[39mfapl)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = '2FBergomiNNWeights_test.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "model_iv.load_weights('2FBergomiNNWeights_test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab55b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "NNParameters=[]\n",
    "for i in range(0,len(model_iv.layers)):\n",
    "    NNParameters.append(model_iv.layers[i].get_weights())\n",
    "\n",
    "len(NNParameters[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d7e1f6",
   "metadata": {},
   "source": [
    "### Transfer the learned NN to Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4308f912",
   "metadata": {},
   "outputs": [],
   "source": [
    "NNParameters=[]\n",
    "for i in range(0,len(model_iv.layers)):\n",
    "    NNParameters.append(model_iv.layers[i].get_weights())\n",
    "\n",
    "NumLayers=3\n",
    "def elu(x):\n",
    "    #Careful function ovewrites x\n",
    "    ind=(x<0)\n",
    "    x[ind]=np.exp(x[ind])-1\n",
    "    return x\n",
    "def eluPrime(y):\n",
    "    # we make a deep copy of input x\n",
    "    x=np.copy(y)\n",
    "    ind=(x<0)\n",
    "    x[ind]=np.exp(x[ind])\n",
    "    x[~ind]=1\n",
    "    return x\n",
    "def NeuralNetwork(x):\n",
    "    input1=x\n",
    "    for i in range(NumLayers):\n",
    "        input1=np.dot(input1,NNParameters[i][0])+NNParameters[i][1]\n",
    "        #Elu activation\n",
    "        input1=elu(input1)\n",
    "    #The output layer is linnear\n",
    "    i+=1\n",
    "    return np.dot(input1,NNParameters[i][0])+NNParameters[i][1]\n",
    "def NeuralNetworkGradient(x):\n",
    "    input1=x\n",
    "    #Identity Matrix represents Jacobian with respect to initial parameters\n",
    "    grad=np.eye(4)\n",
    "    #Propagate the gradient via chain rule\n",
    "    for i in range(NumLayers):\n",
    "        input1=(np.dot(input1,NNParameters[i][0])+NNParameters[i][1])\n",
    "        grad=(np.einsum('ij,jk->ik', grad, NNParameters[i][0]))\n",
    "        #Elu activation\n",
    "        grad*=eluPrime(input1)\n",
    "        input1=elu(input1)\n",
    "    #input1.append(np.dot(input1[i],NNParameters[i+1][0])+NNParameters[i+1][1])\n",
    "    grad=np.einsum('ij,jk->ik',grad,NNParameters[i+1][0])\n",
    "    #grad stores all intermediate Jacobians, however only the last one is used here as output\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4040f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19da33fb-60f5-4564-b987-842cacc35a7c",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e266d5cd-6ed2-42a0-bd08-107ebfbfc729",
   "metadata": {},
   "source": [
    "1)  Lorenzo Bergomi. Smile dynamics II. Risk Magazine, 2005\n",
    "\n",
    "2) Blanka Horvath, Aitor Muguruza, and Mehdi Tomas. Deep learning volatility: a deep neural \n",
    "network perspective on pricing and calibration in (rough) volatility models. Quantitativ \n",
    "Finance, 21(1):11–27, 20218\n",
    "\n",
    "3) Ryan McCrickerd and Mikko S Pakkanen. Turbocharging monte carlo pricing for the rough \n",
    "bergomi model. Quantitative Finance, 18(11):1877–1886, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa50453-b3dd-4f2d-90c5-6f316e215196",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
